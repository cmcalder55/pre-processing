{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"id":"ucfJ2kkbqiQ3"},"outputs":[{"data":{"text/plain":["<spacy.lang.en.English at 0x17ddeed4860>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["\n","import json\n","import spacy\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pylab as plt\n","import nltk\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from typing import List\n","from string import punctuation\n","from scipy.stats import wilcoxon\n","\n","# !python -m spacy download en_core_web_sm\n","spacy.load('en_core_web_sm')\n"]},{"cell_type":"markdown","metadata":{"id":"mXnVLAxL9Is6"},"source":["# Text Processing"]},{"cell_type":"markdown","metadata":{"id":"OtTDLl8vqu7b"},"source":["## TF-IDF"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"zS0LkjMZqwbI"},"outputs":[],"source":["\n","def gen_tfidf(texts, min_df=1.0, max_df=1.0, ngram_range=(1, 1)):\n","    \"\"\"\n","    texts: a list of strings\n","    \"\"\"\n","\n","    # using default tokenizer in TfidfVectorizer\n","    tfidf = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n","\n","    features = tfidf.fit_transform(texts)\n","\n","    df = pd.DataFrame(\n","                    features.todense(),\n","                    columns=tfidf.get_feature_names_out()\n","                    )\n","    return df\n"]},{"cell_type":"markdown","metadata":{},"source":["### examples"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":260},"executionInfo":{"elapsed":823,"status":"ok","timestamp":1711325252740,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"njfQLuWa46B8","outputId":"7c17fbcf-37ea-49ef-f10c-27cb9b04e631"},"outputs":[{"ename":"NameError","evalue":"name 'TfidfVectorizer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m ng \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term, args \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mgen_tfidf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(term\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n","Cell \u001b[1;32mIn[11], line 7\u001b[0m, in \u001b[0;36mgen_tfidf\u001b[1;34m(texts, min_df, max_df, ngram_range)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mtexts: a list of strings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# using default tokenizer in TfidfVectorizer\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfVectorizer\u001b[49m(min_df\u001b[38;5;241m=\u001b[39mmin_df, max_df\u001b[38;5;241m=\u001b[39mmax_df, ngram_range\u001b[38;5;241m=\u001b[39mngram_range)\n\u001b[0;32m      9\u001b[0m features \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(texts)\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     12\u001b[0m                 features\u001b[38;5;241m.\u001b[39mtodense(),\n\u001b[0;32m     13\u001b[0m                 columns\u001b[38;5;241m=\u001b[39mtfidf\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     14\u001b[0m                 )\n","\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"]}],"source":["\n","bi =  {\"min_df\": 2, \"max_df\": 0.5, \"ngram_range\": (1,2)}\n","uni = {\"min_df\": 2, \"max_df\": 3}\n","texts = [\n","    ([\"good movie\", \"not a good movie\", \"did not like\", \"i like it\", \"good one\"], \n","        ((\"good movie\", bi), (\"good\", uni), (\"movie\", uni))),\n","    (['a great film', 'great cast', 'a pleasure to watch', 'not good', 'hard to watch', 'boring film'],\n","        ((\"film\", uni),))\n","]\n","\n","for text, params in texts:\n","    ng = \"\"\n","    for term, args in params:\n","        df = gen_tfidf(text, **args)\n","        ngrams = \"\"\n","        n = len(term.split(\" \"))\n","        res = f\"{n}-gram sum('{term}'): {df[term].sum():.4}\\n\"\n","        ngrams += res\n","    \n","        ng += f\"{'-'*80}\\n{df}\\n{ngrams}\\n\"    \n","    print(ng)\n"]},{"cell_type":"markdown","metadata":{"id":"4EBrDf28QwSI"},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"eEuvTzveR69L"},"outputs":[],"source":["\n","class Tokenizer():\n","    def __init__(self, mode=\"basic\"):\n","        self.mode = mode\n","        self.vocab = {}\n","        self.tokenized = None\n","\n","    def tokenize_basic(self, docs, reset_vocabulary=False):\n","        \"\"\"Tokenizes a sentence to get tokens and vocab freq dict\n","        * accepts a sentence (i.e., `text` parameter) as an input\n","        * splits the sentence into a list of tokens by **space**\n","        * removes the **leading/trailing punctuations or spaces** of each token, if any\n","        * only keeps tokens with 2 or more characters, i.e. `len(token)>1`\n","        * converts all tokens into lower case\n","        * find the count of each unique token and save the counts as dictionary, i.e., `{world: 1, a: 1, ...}`\n","        :param str text: sentence (corpus)\n","        \"\"\"\n","\n","        punct = punctuation + '\\u201c\\u201d\\u2018\\u2019'\n","        # Remove any leading or trailing punctuation and spaces/tabs/new lines from each\n","        # If a word is longer than 2 letters, covert to lower case and keep\n","        tokens = [\n","            [t.lower().strip(punct) for t in doc.split() if len(t) > 1]\n","            for doc in docs\n","        ]\n","        # Store word count in vocab dict\n","        vocab = {}\n","        for doc in tokens:\n","            for t in doc:\n","                if t not in vocab:\n","                    vocab[t] = 1.0\n","                else:\n","                    vocab[t] += 1.0\n","            if reset_vocabulary:\n","                self.vocab = vocab\n","            else:\n","                self.vocab.update(vocab)\n","        \n","        self.vocab = dict(sorted(self.vocab.items(), key=lambda v: v[1], reverse=True))\n","        \n","        return tokens\n","\n","    def tokenize_doc(self, doc, nlp, lemmatized=True, remove_stopword=True, remove_punct=True):\n","        clean_tokens = []\n","        # load current doc into spacy nlp model and split sentences by newline chars\n","        sentences = doc.split(\"\\\\n\")\n","        for sentence in sentences:\n","            doc = nlp(sentence)\n","\n","            # clean either lemmatized unigrams or unmodified doc tokens\n","            if lemmatized:\n","                clean_tokens += [token.lemma_.lower() for token in doc            # using spacy nlp params, skip token if:\n","                                if (not remove_stopword or not token.is_stop)     # it is a stopword and remove_stopwords = True\n","                                and (not remove_punct or not token.is_punct)      # it is punctuation and remove_punct = True\n","                                and not token.lemma_.isspace()]                   # it is whitespace\n","            else:\n","                clean_tokens += [token.text.lower() for token in doc\n","                                if (not remove_stopword or not token.is_stop)\n","                                and (not remove_punct or not token.is_punct)\n","                                and not token.text.isspace()]\n","\n","        return clean_tokens\n","\n","    def tokenize_spacy(self, docs, lemmatized=True, remove_stopword=True, remove_punct=True):\n","        \"\"\"Tokenize documents using methods from the SpaCy library.\n","        (ref: https://spacy.io/api/token#attributes)\n","        \n","        Splits each input document into unigrams and also clean up tokens as follows:\n","        - if `lemmatized` is turned on, lemmatize all unigrams.\n","        - if `remove_stopword` is set to True, remove all stop words.\n","        - if `remove_punct` is set to True, remove all punctuation tokens.\n","        - remove all empty tokens and lowercase all the tokens.\n","        \n","        Parameters:\n","            docs (List[str]): a list of documents.\n","            lemmatized (bool): optional parameter to indicate if tokens are lemmatized. Defaults to True.\n","            remove_stopword(bool): optional parameter to remove stop words. Defaults to True.\n","            remove_punct(bool): optional parameter to remove punctuation. Defaults to True.\n","  \n","        Returns:\n","            out (list): tokens obtained for each document after all the processing.\n","        \"\"\"\n","\n","        # load in spacy NLP model and disable unused pipelines to reduce processing time/memory space\n","        nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n","        nlp.add_pipe(\"sentencizer\")\n","        # tokenize each doc in the corpus using specified params for lemmatization and removal conditions\n","        tokens = [self.tokenize_doc(doc, nlp, lemmatized, remove_stopword, remove_punct) for doc in docs]\n","        return tokens\n","\n","    def tokenize(self, docs, **kwargs):\n","        \n","        tokens = []\n","        if self.mode == \"basic\":\n","            tokens = self.tokenize_basic(docs, **kwargs)\n","        elif self.mode == \"spacy\":\n","            tokens = self.tokenize_spacy(docs, **kwargs)\n","        self.tokenized = tokens\n"]},{"cell_type":"markdown","metadata":{},"source":["### examples"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## example 1 ##\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# basic tokenizer\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/sents.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[0;32m      6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(docs)\n","\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["\n","## example 1 ##\n","# basic tokenizer\n","docs = pd.read_csv(\"../data/sents.csv\")[\"text\"]\n","\n","tokenizer = Tokenizer()\n","tokenizer.tokenize(docs)\n","\n","print(f\"{docs[:5]}\\n\\n{json.dumps(tokenizer.vocab, indent=4)}\")\n","\n","## example 2 ##\n","data = pd.read_csv(\"../data/qa.csv\")\n","\n","# build parameter dicts\n","params = [\n","        (True, False, True),\n","        (True, True, True),\n","        (False, False, True),\n","        (False, False, False)     \n","    ]\n","params = [dict(zip([\"lemmatized\", \"remove_stopword\", \"remove_punct\"], par)) for par in params]\n","\n","# get example document\n","docs = data[\"question\"].iloc[0:1]\n","print(f\"Original document:\\n{docs[0]}\\n\")\n","\n","# tokenize\n","tokenizer = Tokenizer(\"spacy\")\n","print(\"Tokenized:\\n\")\n","for param_tuple in params:\n","    tokenizer.tokenize(docs, **param_tuple)\n","    print(f\"{param_tuple}:\\n{tokenizer.tokenized}\\n\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Performance Evaluation (Precision and Recall)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","def evaluate_performance(prob, truth, th):\n","    \"\"\"Compares the prediction with the ground truth labels to calculate\n","    the confusion matrix as [[TN, FN],[FP,TP]], where:\n","    * True Positives (TP): the number of correct positive predictions\n","    * False Positives (FP): the number of postive predictives which actually are negatives\n","    * True Negatives (TN): the number of correct negative predictions\n","    * False Negatives (FN): the number of negative predictives which actually are positives\n","\n","    :return precision: TP/(TP+FP)\n","    :return recall: TP/(TP+FN)\n","    \"\"\"\n","    \n","    conf = [[0, 0], [0, 0]]\n","    classifiers = list(zip(prob, truth))\n","\n","    for p,t in classifiers:\n","        guess = 0\n","        if p > th:\n","            guess = 1\n","        if guess == t:\n","                if guess == 0:\n","                    conf[0][0] += 1\n","                else:\n","                    conf[1][1] += 1\n","        else:\n","            if guess == 0:\n","                conf[0][1] += 1\n","            else:\n","                conf[1][0] += 1\n","\n","    [[TN,FN],[FP,TP]] = conf\n","    prec = TP/(TP+FP)\n","    rec = TP/(TP+FN)\n","\n","    performance = {\n","        \"R0\": prec,\n","        \"R1\": rec\n","    }\n","    return performance\n","\n","def bigram_precision_recall(gen_tokens, ref_tokens):\n","    result = pd.DataFrame(columns = ['overlapping','precision','recall'])\n","\n","    gen_bigrams = [list(nltk.bigrams(tokens)) for tokens in gen_tokens]\n","    ref_bigrams = [list(nltk.bigrams(tokens)) for tokens in ref_tokens]\n","\n","    bigrams = list(zip(gen_bigrams, ref_bigrams))\n","\n","    overlapping = []\n","    precision = []\n","    recall = []\n","    for gen, ref in bigrams:\n","        overlap = [tup1 for tup1 in gen for tup2 in ref if tup1 == tup2]\n","        overlapping.append(list(set(overlap)))\n","\n","        prec = 0\n","        if gen:\n","            prec = precision.append(len(overlap)/len(gen))\n","        precision.append(prec)\n","\n","        rec = 0\n","        if ref:\n","            rec = recall.append(len(overlap)/len(ref))\n","        recall.append(rec)\n","\n","    result['overlapping'] = overlapping\n","    result['precision'] = precision\n","    result['recall'] = recall\n","\n","    return result\n"]},{"cell_type":"markdown","metadata":{},"source":["### examples"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prob \u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.28997326\u001b[39m, \u001b[38;5;241m0.10166073\u001b[39m, \u001b[38;5;241m0.10759583\u001b[39m, \u001b[38;5;241m0.0694934\u001b[39m , \u001b[38;5;241m0.6767239\u001b[39m ,\n\u001b[0;32m      2\u001b[0m        \u001b[38;5;241m0.01446897\u001b[39m, \u001b[38;5;241m0.15268748\u001b[39m, \u001b[38;5;241m0.15570522\u001b[39m, \u001b[38;5;241m0.12159665\u001b[39m, \u001b[38;5;241m0.22593857\u001b[39m,\n\u001b[0;32m      3\u001b[0m        \u001b[38;5;241m0.98162019\u001b[39m, \u001b[38;5;241m0.47418329\u001b[39m, \u001b[38;5;241m0.09376987\u001b[39m, \u001b[38;5;241m0.80440782\u001b[39m, \u001b[38;5;241m0.88361167\u001b[39m,\n\u001b[0;32m      4\u001b[0m        \u001b[38;5;241m0.21579844\u001b[39m, \u001b[38;5;241m0.72343069\u001b[39m, \u001b[38;5;241m0.06605903\u001b[39m, \u001b[38;5;241m0.15447797\u001b[39m, \u001b[38;5;241m0.10967575\u001b[39m,\n\u001b[0;32m      5\u001b[0m        \u001b[38;5;241m0.93020135\u001b[39m, \u001b[38;5;241m0.06570391\u001b[39m, \u001b[38;5;241m0.05283854\u001b[39m, \u001b[38;5;241m0.09668829\u001b[39m, \u001b[38;5;241m0.05974545\u001b[39m,\n\u001b[0;32m      6\u001b[0m        \u001b[38;5;241m0.04874688\u001b[39m, \u001b[38;5;241m0.07562255\u001b[39m, \u001b[38;5;241m0.11103822\u001b[39m, \u001b[38;5;241m0.71674525\u001b[39m, \u001b[38;5;241m0.08507381\u001b[39m,\n\u001b[0;32m      7\u001b[0m        \u001b[38;5;241m0.630128\u001b[39m  , \u001b[38;5;241m0.16447478\u001b[39m, \u001b[38;5;241m0.16914903\u001b[39m, \u001b[38;5;241m0.1715767\u001b[39m , \u001b[38;5;241m0.08040751\u001b[39m,\n\u001b[0;32m      8\u001b[0m        \u001b[38;5;241m0.7001173\u001b[39m , \u001b[38;5;241m0.04428363\u001b[39m, \u001b[38;5;241m0.19469664\u001b[39m, \u001b[38;5;241m0.12247959\u001b[39m, \u001b[38;5;241m0.14000294\u001b[39m,\n\u001b[0;32m      9\u001b[0m        \u001b[38;5;241m0.02411263\u001b[39m, \u001b[38;5;241m0.26276603\u001b[39m, \u001b[38;5;241m0.11377073\u001b[39m, \u001b[38;5;241m0.07055441\u001b[39m, \u001b[38;5;241m0.2021157\u001b[39m ,\n\u001b[0;32m     10\u001b[0m        \u001b[38;5;241m0.11636899\u001b[39m, \u001b[38;5;241m0.90348488\u001b[39m, \u001b[38;5;241m0.10191679\u001b[39m, \u001b[38;5;241m0.88744523\u001b[39m, \u001b[38;5;241m0.18938904\u001b[39m])\n\u001b[0;32m     12\u001b[0m truth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     13\u001b[0m                 \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     14\u001b[0m                 \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     15\u001b[0m                 \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     16\u001b[0m                 \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Test with threhold grid varying from 0.05 to 0.95 with an increase of 0.05\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"]}],"source":["\n","prob =np.array([0.28997326, 0.10166073, 0.10759583, 0.0694934 , 0.6767239 ,\n","       0.01446897, 0.15268748, 0.15570522, 0.12159665, 0.22593857,\n","       0.98162019, 0.47418329, 0.09376987, 0.80440782, 0.88361167,\n","       0.21579844, 0.72343069, 0.06605903, 0.15447797, 0.10967575,\n","       0.93020135, 0.06570391, 0.05283854, 0.09668829, 0.05974545,\n","       0.04874688, 0.07562255, 0.11103822, 0.71674525, 0.08507381,\n","       0.630128  , 0.16447478, 0.16914903, 0.1715767 , 0.08040751,\n","       0.7001173 , 0.04428363, 0.19469664, 0.12247959, 0.14000294,\n","       0.02411263, 0.26276603, 0.11377073, 0.07055441, 0.2021157 ,\n","       0.11636899, 0.90348488, 0.10191679, 0.88744523, 0.18938904])\n","\n","truth = np.array([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n","                1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n","                0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n","                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","                0, 0, 1, 0, 1, 0])\n","\n","# Test with threhold grid varying from 0.05 to 0.95 with an increase of 0.05\n","th_list = np.arange(0.05, 1.00, 0.05)\n","\n","vals = [tuple(evaluate_performance(prob, truth, th).values()) for th in th_list]\n","prec, rec = zip(*vals)\n","vals_df = pd.DataFrame(vals, columns = ['prec', 'rec'], index = th_list)\n","result = evaluate_performance(prob, truth, 0.05)\n","\n","print(f\"Precision/Recall\\n\\nOne Value Only:\\n{json.dumps(result, indent=4)}\\n\")\n","print(f\"All Values:\\n{vals_df}\\n\")\n","\n","plt.plot(th_list, vals_df, label = ['prec','rec'])\n","plt.title('Precision and Recall vs. Threshold')\n","plt.xlabel('Threshold')\n","plt.ylabel('Precision and Recall')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"1iEN6xGaS_6a"},"source":["## Determine Sentiment"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9FNQWLi0S_Vb"},"outputs":[],"source":["\n","def compute_sentiment(target, pos, neg):\n","    p = sum(1 for word in target if word in pos)\n","    n = sum(1 for word in target if word in neg)\n","    if p + n != 0:\n","        sentiment = (p - n) / (p + n)\n","    else:\n","        sentiment = 0\n","    return sentiment\n","\n","def sentiment(gen_tokens, ref_tokens, pos, neg):\n","\n","    tokens = lambda token_list: [compute_sentiment(sublist, pos, neg) for sublist in token_list]\n","    result = pd.DataFrame({'gen_sentiment': tokens(gen_tokens), 'ref_sentiment': tokens(ref_tokens)})\n","\n","    avg = (result['gen_sentiment'] - result['ref_sentiment']).mean()\n","    res = wilcoxon(result['gen_sentiment'] - result['ref_sentiment'], alternative='greater')\n","\n","    print(f\"Average Sentiment: {avg}\\n\")\n","    print(f\"Stat: {res.statistic}\\nP-Value: {res.pvalue}\\n\")\n","    return result\n"]},{"cell_type":"markdown","metadata":{},"source":["### examples"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1711332709813,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"um3ThIHATPsM","outputId":"fe4ebd0c-45e9-42c5-9cb2-63d788f9b98a"},"outputs":[{"ename":"TypeError","evalue":"Tokenizer.__init__() got an unexpected keyword argument 'select'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspacy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m pos_words \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/positive-words.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m pos \u001b[38;5;241m=\u001b[39m pos_words[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n","\u001b[1;31mTypeError\u001b[0m: Tokenizer.__init__() got an unexpected keyword argument 'select'"]}],"source":["\n","tokenizer = Tokenizer(select=\"spacy\")\n","\n","pos_words = pd.read_csv(\"../data/positive-words.txt\", header=None)\n","pos = pos_words[0].values\n","\n","neg_words = pd.read_csv(\"../data/negative-words.txt\", header=None)\n","neg = neg_words[0].values\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":159398,"status":"ok","timestamp":1711332944128,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"AFBXGtnNWCNa","outputId":"ebfd98c5-7d07-45f4-8af5-57d9ae3f4bac"},"outputs":[{"ename":"TypeError","evalue":"Tokenizer.tokenize_basic() got an unexpected keyword argument 'lemmatized'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[27], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m ref \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lemmatized, stopword, punct \u001b[38;5;129;01min\u001b[39;00m combos:\n\u001b[1;32m---> 12\u001b[0m     gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlemmatized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stopword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_punct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpunct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     ref_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(ref, lemmatized\u001b[38;5;241m=\u001b[39mlemmatized, remove_stopword\u001b[38;5;241m=\u001b[39mstopword, remove_punct\u001b[38;5;241m=\u001b[39mpunct)\n\u001b[0;32m     15\u001b[0m     text_sentiment \u001b[38;5;241m=\u001b[39m sentiment(gen_tokens, ref_tokens, pos, neg)\n","Cell \u001b[1;32mIn[17], line 90\u001b[0m, in \u001b[0;36mTokenizer.tokenize\u001b[1;34m(self, docs, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     92\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_spacy(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[1;31mTypeError\u001b[0m: Tokenizer.tokenize_basic() got an unexpected keyword argument 'lemmatized'"]}],"source":["\n","combos = [(True, False, False),\n","            (True, True, False),\n","            (True, False, True),\n","            (True, True, True),\n","            (False, False, True),\n","            (False, False, False)]\n","\n","gen = data[\"chatgpt_answer\"]\n","ref = data[\"human_answer\"]\n","\n","for lemmatized, stopword, punct in combos:\n","    gen_tokens = tokenizer.tokenize(gen, lemmatized=lemmatized, remove_stopword=stopword, remove_punct=punct)\n","    ref_tokens = tokenizer.tokenize(ref, lemmatized=lemmatized, remove_stopword=stopword, remove_punct=punct)\n","\n","    text_sentiment = sentiment(gen_tokens, ref_tokens, pos, neg)\n","    result = bigram_precision_recall(gen_tokens, ref_tokens)\n","    \n","    print(f\"lemmatized={lemmatized}, remove_stopword={stopword}, remove_punct={punct}\")\n","    print(result[[\"precision\", \"recall\"]].mean(axis = 0), text_sentiment.describe())\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"ogz5r9wfRLVQ"},"source":["## Document term matrix (DTM)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TZomUJSPUYq6"},"outputs":[],"source":["\n","def get_dtm(sents):\n","    \"\"\"\n","    - accepts a list of sentences, i.e., `sents`, as an input\n","    - call `tokenize` function you defined in Q1 to get the count dictionary for each sentence, and combine them into a list\n","    - call `generate_vocab` function in Q2 to generate the large vocabulary for all sentences, and get all the words, i.e., keys\n","    - creates a numpy array, say `dtm` with a shape (# of docs x # of unique words), and set the initial values to 0.\n","    - fills cell `dtm[i,j]` with the count of the `j`th word in the `i`th sentence. HINT: you can loop through the list of \n","    vocabulary from step 2, and check each word's index in the large vocabulary from step 3, so that you can put the \n","    corresponding value into the correct cell.\n","    - returns `dtm` and `unique_words`\n","    \"\"\"\n","\n","    tokenizer = Tokenizer(\"spacy\")\n","    tokenizer.tokenize(sents, lemmatized=True, remove_stopword=True, remove_punct=True)\n","    \n","    all_docs = tokenizer.tokenized\n","    all_words = tokenizer.vocab\n","\n","    m,n = len(all_docs), len(all_words)\n","    dtm = np.zeros((m,n))\n","\n","    for doc in range(m):\n","        for i, word in enumerate(all_words.keys()):\n","            if word in all_docs[doc]:\n","                dtm[doc,i] = all_docs[doc][word]\n","\n","    return dtm, all_words, tokenizer\n","\n","def analyze_dtm(dtm, words, sents):\n","    \"\"\"\n","    * takes an array $dtm$ and $words$ as an input, where $dtm$ is the array you get in Q3 with a shape $(m times n)$,\n","    and $words$ contains an array of words corresponding to the columns of $dtm$.\n","    * calculates the sentence frequency for each word, say $j$, e.g. how many sentences contain word $j$. Save the result\n","    to array $df$ ($df$ has shape of $(n,)$ or $(1, n)$).\n","    * normalizes the word count per sentence: divides word count, i.e., $dtm_{i,j}$, by the total number of words in\n","    sentence $i$. Save the result as an array named $tf$ ($tf$ has shape of $(m,n)$).\n","    * for each $dtm_{i,j}$, calculates $tf/_idf_{i,j} = frac{tf_{i, j}}{df_j}$, i.e., divide each normalized word count by \n","    the sentence frequency of the word. The reason is, if a word appears in most sentences, it does not have the discriminative \n","    power and often is called a `stop` word. The inverse of $df$ can downgrade the weight of such words. $tf/_idf$ has shape of $(m,n)$\n","    * prints out the following:\n","\n","        - the total number of words in the document represented by $dtm$\n","        - the most frequent top 10 words in this document, compare with the results from Q2, and briefly explain the difference\n","        - words with the top 10 largest $df$ values (show words and their $df$ values)\n","        - the longest sentence (i.e., the one with the most words)\n","        - top-10 words with the largest $tf/_idf$ values in the longest sentence (show words and values)\n","    * returns the $tf/_idf$ array.\n","\n","    Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation.\n","    \"\"\"\n","\n","    df = np.count_nonzero(dtm, axis=0)\n","\n","    total = dtm.sum(axis=1)[:, np.newaxis]\n","    tf = dtm/total\n","\n","    tfidf = tf/df[np.newaxis,:]\n","\n","    n_top = 10\n","\n","    words_freq = dtm.sum(axis=0)\n","    words_most = words_freq.argsort()[::-1][:n_top]\n","    top_words = list(zip(words[words_most], words_freq[words_most]))\n","\n","    hi_df = df.argsort()[::-1][:n_top]\n","    top_df = list(zip(words[hi_df], df[hi_df]))\n","\n","    longest = dtm.sum(axis=1).argmax()\n","\n","    longest_tfidf = tfidf[longest].argsort()[::-1][:n_top]\n","    top_tfidf = list(zip(words[longest_tfidf], tfidf[longest][longest_tfidf]))\n","\n","    print(f'The total number of words:\\n{dtm.sum()}\\n')\n","    print(f'The top 10 frequent words:\\n{top_words}\\n')\n","    print(f'The top 10 words with highest df values:\\n{top_df}\\n')\n","    print(f'The longest sentence:\\n{sents[longest]}\\n')\n","    print(f'The top 10 words with highest tf-idf values in the longest sentence:\\n{top_tfidf}')\n","\n","    return tfidf\n"]},{"cell_type":"markdown","metadata":{},"source":["### examples"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"oUAlC6h2Q-Pr"},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sents \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/sents.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m dtm, all_words, tokenizer \u001b[38;5;241m=\u001b[39m get_dtm(sents[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# randomly check one sentence\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["\n","# TODO\n","sents = pd.read_csv(\"../data/sents.csv\")\n","dtm, all_words, tokenizer = get_dtm(sents[\"text\"])\n","\n","# randomly check one sentence\n","idx = 3\n","\n","# get the dictionary using the tokenizer\n","vocab = tokenizer.tokenized[idx]\n","\n","# get all non-zero entries in dtm[idx] and create a dictionary\n","vocab_dtm = {all_words[j]: dtm[idx][j] for j in np.where(dtm[idx]>0)[0]}\n","\n","print(vocab, vocab_dtm)\n","\n","a = sorted(vocab.items(), key = lambda item: item[0])\n","b = sorted(vocab_dtm.items(), key = lambda item: item[0])\n","# Check if the array is correct\n","assert a == b, \"Dicts don't match!\"\n","\n","sents.loc[idx]\n","\n","print(a, b)\n","\n","# analyze dtm array\n","analyze_dtm(dtm, np.array(all_words), sents.text)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Text Processor"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# TODO\n","class TextProcessor:\n","    def __init__(self, \n","                 tokenizer: Tokenizer=Tokenizer(),\n","                 tfidf_vectorizer: TfidfVectorizer=TfidfVectorizer(),\n","                 bert_vectorizer=None,\n","                 bow_vectorizer=None,\n","                 w2v_vectorizer=None\n","            ):\n","        # tokenizer attrs\n","        self.tokenizer = tokenizer\n","        self.tokenized = None\n","        self.vocabulary = None\n","        # tf-idf attrs\n","        self.tfidf_vectorizer = tfidf_vectorizer\n","        self.tf_idf = None\n","        self.dtm = None\n","        # other vectorizers\n","        self.bert_vectorizer = bert_vectorizer\n","        self.bow_vectorizer = bow_vectorizer\n","        self.w2v_vectorizer = w2v_vectorizer\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMQn9WXp7A4fckSgJdYAEPs","collapsed_sections":["taps-i_p35Pm","_D9ZYWHfqmq_","OtTDLl8vqu7b","klwhLA3JRILI","ogz5r9wfRLVQ","PPTky0xgRFjX"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
