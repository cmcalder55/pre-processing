{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1711905521093,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"GoaHUm70yyMK"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'nltk'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"]}],"source":["\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import gzip\n","import torch\n","\n","import tqdm as notebook_tqdm\n","from string import punctuation\n","from sklearn.model_selection import train_test_split\n","from gensim.models import Word2Vec, KeyedVectors\n","from transformers import BertTokenizer, BertModel, pipeline, AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset\n","from datasets import load_dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"b-RtWorSz--0"},"source":["# Bag of Words\n","\n","There are many ways to transform text data to numeric vectors. In this task you will try to use two of them. One of the well-known approaches is a bag-of-words representation. To create this transformation, follow the steps:\n","\n","1. Find N most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.  \n","2. For each title in the corpora create a zero vector with the dimension equals to N.  \n","3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9L0O0uuufoRh"},"outputs":[],"source":["\n","\n","def my_bag_of_words(text, words_to_index):\n","    \"\"\"\n","    text: a string\n","    words_to_index: a list, train corpus words\n","    dict_size: size of the dictionary\n","\n","    return a vector which is a bag-of-words representation of 'text'\n","    \"\"\"\n","\n","    dict_size = len(words_to_index)\n","\n","    result_vector = np.zeros(dict_size)\n","\n","    popular_words = enumerate(set(words_to_index))\n","    words_idx = {w:i for i,w in popular_words}\n","\n","    for text in text.split():\n","        if text in words_idx:\n","            result_vector[words_idx[text]] += 1\n","\n","    return result_vector\n","\n","text = 'hi how are you'\n","words_to_index = ['hi', 'you', 'me', 'are']\n","\n","my_bag_of_words(text, words_to_index)\n"]},{"cell_type":"markdown","metadata":{"id":"RVCS-f9lz3g0"},"source":["# Word Vectorization"]},{"cell_type":"markdown","metadata":{"id":"SS30pMyrfhtO"},"source":["## Word2Vector  \n","\n","- Google published pre-trained 300-dimensional vectors for 3 million words and phrases that were trained on Google News dataset (about 100 billion words)(https://code.google.com/archive/p/word2vec/)\n","- GloVe (Global Vectors for Word Representation): Pretained word vectors from different data sources provided by Standford https://nlp.stanford.edu/projects/glove/\n","- FastText by Facebook https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ohk8r_hyynHg"},"outputs":[],"source":["\n","def train_wordvec(docs, vector_size):\n","    # Tokenize docs into tokens and remove punctuations\n","    documents = []\n","    for doc in docs:\n","        tokens = [word.strip(punctuation) for word in doc.lower().split()]\n","        documents.append(tokens)\n","\n","    # Train word vectors using gensim package\n","    model = Word2Vec(documents, vector_size=vector_size, window=5, min_count=1, workers=4)\n","\n","    return model\n","\n","def generate_doc_vector(docs, wv_model):\n","\n","\n","    # Tokenize each document into tokens\n","    tokenized_docs = []\n","    for doc in docs:\n","        tokens = [word.strip(punctuation) for word in doc.lower().split()]\n","        tokenized_docs.append(tokens)\n","\n","    # Generate document vectors\n","    doc_vectors = []\n","    for doc in tokenized_docs:\n","        vectors = [wv_model.wv[token] for token in doc if token in wv_model.wv]\n","        if len(vectors) > 0:\n","            doc_vector = np.mean(vectors, axis=0)\n","        else:\n","            doc_vector = np.zeros(wv_model.vector_size)\n","        doc_vectors.append(doc_vector)\n","\n","    vectors = np.array(doc_vectors)\n","    # Return document vectors as a numpy array\n","    return vectors\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_MWnh7aytvT"},"outputs":[],"source":["# Here we will use our previous chatgpt dataset\n","data=pd.read_csv(\"./data/detect.csv\")\n","x_train, x_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2,random_state=0)\n","\n","data.head()\n","\n","documents=[\n","            [\n","                token.strip(punctuation).strip()\n","                 for token in nltk.word_tokenize(doc.lower())\n","                    if token not in punctuation and len(token.strip(punctuation).strip()) >= 2\n","            ]\n","            for doc in data[\"text\"]\n","        ]\n","\n","# use function\n","model = Word2Vec(documents, vector_size=300, window=5, min_count=5, workers=4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711860021978,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"imtS7jNayuvx","outputId":"ebd8cb69-659b-4e55-c0ba-4c449f9fa17f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('dance', 0.8091345429420471), ('pop', 0.8018935918807983), ('scene', 0.8004364371299744), ('recording', 0.7784169912338257), ('soul', 0.77668297290802)] \n","\n","[('mix', 0.6810025572776794), ('brass', 0.6776862144470215), ('acoustic', 0.6772572994232178), ('punk', 0.6729581356048584), ('influences', 0.664824903011322)] \n","\n","0.77276725 \n","\n","-0.11576279 \n","\n","actor\n"]}],"source":["\n","# print(model.wv['movie'], '\\n')\n","\n","# Top {n} word(s) with high relevance to {positive_vector} and low relevance to {negative_vector}: {top_n}\n","wv_correlation = lambda positive_vector=[], negative_vector=[], n=5: model.wv.most_similar(positive=positive_vector, negative=negative_vector, topn=n)\n","print(wv_correlation(['sound','music']), '\\n')\n","print(wv_correlation(['sound','music'], ['film']), '\\n')\n","\n","\n","# similarity between two tokens\n","wv_pos_sim = lambda token_a, token_b: model.wv.similarity(token_a, token_b)\n","print(wv_pos_sim('brass', 'acoustic'), '\\n')\n","print(wv_pos_sim('movie','city'), '\\n')\n","\n","wv_outlier = lambda word_vector: model.wv.doesnt_match(word_vector)\n","print(wv_outlier([\"sound\", \"music\", \"graphics\", \"actor\", \"book\"]))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# from https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin\n","\n","bin_url = \"GoogleNews-vectors-negative300.bin\"\n","gz_url = 'GoogleNews-vectors-negative300.bin.gz'\n","url = f'https://s3.amazonaws.com/dl4j-distribution/{gz_url}'\n","\n","filename = wget.download(url)\n","\n","f_in = gzip.open(gz_url, 'rb')\n","f_out = open(bin_url, 'wb')\n","f_out.writeline(f_in)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113309,"status":"ok","timestamp":1711906031657,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"5cdQNykOgvtz","outputId":"51ee8a5e-3f6d-4cb2-a1df-1119096da0e5"},"outputs":[{"data":{"text/plain":["[('queen', 0.4827326238155365),\n"," ('queens', 0.466781347990036),\n"," ('kumaris', 0.4653734564781189),\n"," ('kings', 0.4558638632297516),\n"," ('womens', 0.422832190990448),\n"," ('princes', 0.4176960587501526),\n"," ('Al_Anqari', 0.41725507378578186),\n"," ('concubines', 0.4011078476905823),\n"," ('monarch', 0.3962482810020447),\n"," ('monarchy', 0.39430150389671326)]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n","model.most_similar(positive=['women','king'], negative='man')\n"]},{"cell_type":"markdown","metadata":{},"source":["## BERT Embedding"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<?, ?B/s]\n","Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 29.3MB/s]\n","Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 35.0MB/s]\n","Downloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 42.8MB/s]\n","Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 198850.03 examples/s]\n","Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 268019.66 examples/s]\n","Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 292416.41 examples/s]\n"]}],"source":["# Load data\n","# data=pd.read_csv('../data/amazon_reviews.csv')\n","# data.columns= ['label','text']\n","\n","# Load a sample dataset (adjust this to your actual data structure)\n","dataset = load_dataset(\"imdb\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    \n","class BertVectorizer(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length \n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        sentences = self.data[idx]['text']  # Assuming data is a list of dicts with 'text' and 'label'\n","        label = self.data[idx]['label']\n","        \n","        # Concatenate sentences\n","        text = \" \".join(sentences)\n","        \n","        # Tokenize and encode the concatenated text\n","        encoding = self.tokenizer(text, \n","                        padding='max_length', \n","                        truncation=True, \n","                        max_length=self.max_length, \n","                        return_tensors='pt'\n","                        )\n","        \n","        # Return input_ids, attention_mask, and label as tensors\n","        model_attrs = { k: v.squeeze(0) for k, v in encoding.items() }\n","        return model_attrs, torch.tensor(label)\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name = \"bert-base-uncased\"\n","\n","# Load pre-trained BERT model with a classification head\n","model = AutoModelForSequenceClassification.from_pretrained(model_name,\n","                                                           output_hidden_states=True,\n","                                                           num_labels=2)\n","\n","# Load pre-trained BERT tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, \n","                                          do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Transform the dataset format to a list of lists of sentences\n","# This step assumes dataset['train'] and dataset['test'] are lists of dictionaries with 'text' and 'label'\n","train_data = [{'text': [example['text']], 'label': example['label']} for example in dataset['train']]\n","test_data = [{'text': [example['text']], 'label': example['label']} for example in dataset['test']]\n","\n","# Create custom dataset instances\n","train_dataset = BertVectorizer(train_data, tokenizer)\n","test_dataset = BertVectorizer(test_data, tokenizer)\n","\n","# Create DataLoaders\n","batch_size = 8\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# Define optimizer and learning rate scheduler\n","lr = 2e-5\n","num_epochs = 3\n","optimizer = AdamW(model.parameters(), lr=lr)\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps=0, \n","                                            num_training_steps=len(train_loader)*num_epochs)\n","\n","# Define training function\n","def train(model, dataloader, optimizer, scheduler):\n","    \n","    model.train()\n","    \n","    total_loss = 0\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        inputs, labels = batch\n","        outputs = model(**inputs, labels=labels)\n","        \n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        loss.backward()\n","        \n","        optimizer.step()\n","        scheduler.step()\n","        \n","    avg_loss = total_loss / len(dataloader)\n","    return avg_loss\n","\n","# Define evaluation function\n","def evaluate(model, dataloader):\n","    \n","    model.eval()\n","    \n","    total_acc = 0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            inputs, labels = batch\n","            outputs = model(**inputs, labels=labels)\n","            \n","            logits = outputs.logits\n","            predictions = torch.argmax(logits, dim=-1)\n","            \n","            acc = (predictions == labels).float().mean()\n","            total_acc += acc.item()\n","            \n","    avg_acc = total_acc / len(dataloader)\n","    return avg_acc\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, optimizer, scheduler)\n","    val_acc = evaluate(model, val_loader)\n","    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Accuracy: {val_acc:.4f}')\n","\n","# Save the model\n","model_filename = \"./tuned_bert\"\n","model.save_pretrained(model_filename)\n","tokenizer.save_pretrained(model_filename)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":667,"referenced_widgets":["9ea4802221d44167a6233ea3175fcd6a","f1a2fae51e9542e1ba58fa7eb03a88f4","8678086282854e7c92a04980aa570c29","78e750c788294fa18229e3ed3922bfd1","7921421d0c5542fc9ac1b4268e15ed03","11fb7e2a02204e749bb7f92ea0044e58","73c9300d2bb74c0eb1cb18c524530219","953da43ba083434184e994edce05c9ac","e0fabf1f050e41e193e7388645396d45","2ed7cbfe49824cc0a27fde4dcfbb8bc2","a3e1706adc6f4ec985194d8e17c88175","e06c96e4e2144460aa1e858c9bda34db","cfd726138ced45eab470bc74f4c7a460","062d165d39eb4893a30f4e5b95cbbeb8","6c2891c69bd344c4bf03bd0be2af900d","ec692ca7d5164313a9ef676a09f40668","4a3e2f0ca4e849b391fc40d531708c7e","d17677e7997c47ee83d8b0f1710d852e","1e9b61b641054998acb41a521558b0a4","de517d37d747469e9466c71f14b334bb","07caa3d01ad64211b0c2faa62678d780","23ef0580534e4e3b9174e823e4336b56","e08a90e438eb4c0e85c9e1ba1d366564","bb7866841ca7485d9408bbc85135431b","8a4d8abd5e0a4046b43adb19dd6671db","97bb383af4214933a21b2c2e69a9accc","6d6da478249b4751bb5c70a2041faa53","c7127db0acf14e7aba570b90837a2131","8d2ee842daa74660b1431c6d93a0bbf9","2b32bf0f9a244383b673cdf96b017905","e0204284f7f24c70822772ec398c7728","7ab65fa365344bbfbab6be79bbfb4c2f","62ba208a1ffc4e0cb794a25cccbbe120","d9795910ee3d467abe13484182aadcbf","f5756a9ea60f47d8a46b143a9d3cdc73","0e550590e8dc4b9a8d851a683f31a32f","87fe1eb308e2471faadb4fdbddaa2884","7e2c78ab77ba4910b3712c1e0af16951","45ab06d8c03a4c97871e45d20dbe6339","3d5e9fe8b29b42a4908fb9ed4b0af914","882d840c38a54e09bd6d681a78214912","546939d1ff514057a3f610445bf1d62b","9e1a8ebec6754f8980e12ae8b5ba96b4","aef9aee7614b4c6b8745d805d262b30b"]},"executionInfo":{"elapsed":21438,"status":"ok","timestamp":1711906161438,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"h3DOzdUPhNFh","outputId":"b91210c6-bb2b-4df9-faba-600946b75029"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading BERT tokenizer...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ea4802221d44167a6233ea3175fcd6a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e06c96e4e2144460aa1e858c9bda34db","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e08a90e438eb4c0e85c9e1ba1d366564","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9795910ee3d467abe13484182aadcbf","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"]},{"data":{"text/plain":["['knight',\n"," 'lap',\n"," 'survey',\n"," 'ma',\n"," '##ow',\n"," 'noise',\n"," 'billy',\n"," '##ium',\n"," 'shooting',\n"," 'guide',\n"," 'bedroom',\n"," 'priest',\n"," 'resistance',\n"," 'motor',\n"," 'homes',\n"," 'sounded',\n"," 'giant',\n"," '##mer',\n"," '150',\n"," 'scenes']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\n","\n","text = \"Here is the sentence I want embeddings for.\"\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","# Tokenize our sentence with the BERT tokenizer\n","tokenized_text = tokenizer.tokenize(marked_text)\n","\n","# Print out the tokens\n","print (tokenized_text)\n","# The original word has been split into smaller subwords and characters.\n","# The two hash signs preceding some of these subwords are just our tokenizer’s way to denote that this subword\n","# or character is part of a larger word and preceded by another subword.\n","# this way some contextual meaning of the original word will be retained.\n","\n","# check out contents of BERT’s vocabulary\n","list(tokenizer.vocab.keys())[5000:5020]\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481,"referenced_widgets":["0d9bfd43ba884154be8976fd48c05cde","f6b2a0f7ace8445ca07357fb54b0df24","b78c6465e67e4470adf7c8d4b8b62986","df58971bf6cd4f4d99d481ab4d2565b6","2e262c27c93642c59ad022d1c3b835e3","d0b3e0e190e0441abb84ac6fd0c7cb0c","e3347193cd6544418fb37f04fa4e39aa","62530bd82b0249639448eb6637726489","ebc0a0d5bc564278adeaa2dcdea84bdb","45714c75b2b0451b97ba48002648c2dc","2c9647540566454c961b4185acb5069d"]},"executionInfo":{"elapsed":10443,"status":"ok","timestamp":1711906259344,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"3g0My0RVjCuy","outputId":"3f2dbb70-a88a-4038-f852-025fe2e75307"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d9bfd43ba884154be8976fd48c05cde","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/plain":["[{'score': 0.3182409107685089,\n","  'token': 2064,\n","  'token_str': 'can',\n","  'sequence': 'artificial intelligence can take over the world.'},\n"," {'score': 0.18299664556980133,\n","  'token': 2097,\n","  'token_str': 'will',\n","  'sequence': 'artificial intelligence will take over the world.'},\n"," {'score': 0.05600148066878319,\n","  'token': 2000,\n","  'token_str': 'to',\n","  'sequence': 'artificial intelligence to take over the world.'},\n"," {'score': 0.04519490897655487,\n","  'token': 2015,\n","  'token_str': '##s',\n","  'sequence': 'artificial intelligences take over the world.'},\n"," {'score': 0.045153163373470306,\n","  'token': 2052,\n","  'token_str': 'would',\n","  'sequence': 'artificial intelligence would take over the world.'}]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["\n","unmasker = pipeline('fill-mask', model='bert-base-uncased')\n","unmasker(\"Artificial Intelligence [MASK] take over the world.\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21460,"status":"ok","timestamp":1711906406514,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"6Uml3v31kOLQ","outputId":"f3981baf-04fc-45a6-df16-cde403aa2cf5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"name":"stdout","output_type":"stream","text":["[['this', 'is', 'little', 'longer', 'and', 'more', 'detailed', 'than', 'the', 'first', 'two', 'books', 'in', 'the', 'series', 'however', 'have', 'enjoyed', 'each', 'new', 'aspect', 'of', 'the', 'exciting', 'fantasy', 'universe'], ['only', 'michelle', 'branch', 'save', 'this', 'album', 'all', 'guys', 'play', 'along', 'with', 'unenthusiastic', 'beat', 'even', 'karl']]\n"]}],"source":["\n","# Load data\n","data=pd.read_csv('/data/amazon_reviews.csv')\n","data.columns=['label','text']\n","data\n","\n","# tokenize each document into a list of unigrams\n","# strip punctuations and leading/trailing spaces from unigrams\n","# only unigrams with 2 or more characters are taken\n","sentences=[ [token.strip(punctuation).strip() \n","             for token in nltk.word_tokenize(doc.lower()) \n","             if token not in punctuation and len(token.strip(punctuation).strip())>=2]\n","        for doc in data[\"text\"] ]\n","\n","print(sentences[0:2])\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1711906410944,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"Psr6sAjVjSlb","outputId":"463d497e-2afc-497f-9305-ab51197b1487"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":[" Original:  This is a little longer and more detailed than the first two books in the series. However, I have enjoyed each new aspect of the exciting fantasy universe.\n","Tokenized:  ['this', 'is', 'a', 'little', 'longer', 'and', 'more', 'detailed', 'than', 'the', 'first', 'two', 'books', 'in', 'the', 'series', '.', 'however', ',', 'i', 'have', 'enjoyed', 'each', 'new', 'aspect', 'of', 'the', 'exciting', 'fantasy', 'universe', '.']\n","Token IDs:  [2023, 2003, 1037, 2210, 2936, 1998, 2062, 6851, 2084, 1996, 2034, 2048, 2808, 1999, 1996, 2186, 1012, 2174, 1010, 1045, 2031, 5632, 2169, 2047, 7814, 1997, 1996, 10990, 5913, 5304, 1012]\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# use our data\n","data=data.iloc[:100]\n","sentences=data[\"text\"].values\n","# Print the original sentence.\n","print(' Original: ', sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n","\n","input_ids = []\n","attention_masks = []\n","max_len =50\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                                       # Sentence to encode\n","                        add_special_tokens = True,                  # Add '[CLS]' and '[SEP]'\n","                        max_length = max_len,                       # Pad & truncate all sentences\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,               # Construct attn. masks\n","                        return_tensors = 'pt',                      # Return pytorch tensors\n","                   )\n","\n","    # Add the encoded sentence to the list.\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16502,"status":"ok","timestamp":1711906430524,"user":{"displayName":"Cameron C.","userId":"04349246908971071571"},"user_tz":240},"id":"xodSHUgojtre","outputId":"a5bbc4e9-f370-4308-cf66-599f1ba24403"},"outputs":[{"name":"stderr","output_type":"stream","text":["We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"name":"stdout","output_type":"stream","text":["Number of layers: 13   (initial embeddings + 12 BERT layers)\n","Number of batches: 100\n","Number of tokens: 50\n","Number of hidden units: 768\n","Number of layers: 13   (initial embeddings + 12 BERT layers)\n","Number of batches: 100\n","Number of tokens: 50\n","Number of hidden units: 768\n","torch.Size([4, 100, 50, 768])\n","torch.Size([100, 50, 4, 768])\n","torch.Size([100, 50, 768])\n"]}],"source":["\n","\n","# Load pre-trained model (weights)\n","bert_model = BertModel.from_pretrained('bert-base-uncased',\n","                                    output_attentions = True, # Whether the model returns attentions weights.\n","                                    output_hidden_states = True, # Whether the model returns all hidden-states.\n","                                  )\n","\n","## Put the model in \"evaluation\" mode, meaning feed-forward operation\n","bert_model.eval()\n","with torch.no_grad():\n","    outputs = bert_model(input_ids)\n","    hidden_states = outputs[2]                #the third item will be the hidden states from all layers\n","\n","print (f\"Number of layers: {len(hidden_states)} (initial embeddings + 12 BERT layers)\")\n","layer_i = 0\n","print (f\"Number of batches: {len(hidden_states[layer_i])}\")\n","#The second dimension, the batch size, is used when submitting multiple sentences to the model at once\n","batch_i = 0\n","print (f\"Number of tokens: {len(hidden_states[layer_i][batch_i])}\")\n","token_i = 0\n","print (f\"Number of hidden units: {len(hidden_states[layer_i][batch_i][token_i])}\")\n","\n","# get the mean of last four layers instead of avging all layers\n","token_embeddings = torch.stack(hidden_states[-4:], dim=0)\n","print(f\"Size of last 4 layers: {token_embeddings.size()}\")\n","token_embeddings = token_embeddings.permute(1, 2, 0, 3)             # permute axis\n","token_embeddings = token_embeddings.mean(axis=2)                    # take the mean of the last 4 layers to get final embeddings\n","print(f\"Size of mean embedding layer: {token_embeddings.size()}\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNi0SyRuvdSNdnkLZDxJ9Rc","mount_file_id":"1sSOaeqi9j3YjJCSOfOLF9chwOBW6lc1g","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"062d165d39eb4893a30f4e5b95cbbeb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e9b61b641054998acb41a521558b0a4","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de517d37d747469e9466c71f14b334bb","value":231508}},"07caa3d01ad64211b0c2faa62678d780":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d9bfd43ba884154be8976fd48c05cde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6b2a0f7ace8445ca07357fb54b0df24","IPY_MODEL_b78c6465e67e4470adf7c8d4b8b62986","IPY_MODEL_df58971bf6cd4f4d99d481ab4d2565b6"],"layout":"IPY_MODEL_2e262c27c93642c59ad022d1c3b835e3"}},"0e550590e8dc4b9a8d851a683f31a32f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_882d840c38a54e09bd6d681a78214912","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_546939d1ff514057a3f610445bf1d62b","value":570}},"11fb7e2a02204e749bb7f92ea0044e58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e9b61b641054998acb41a521558b0a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23ef0580534e4e3b9174e823e4336b56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b32bf0f9a244383b673cdf96b017905":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c9647540566454c961b4185acb5069d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e262c27c93642c59ad022d1c3b835e3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed7cbfe49824cc0a27fde4dcfbb8bc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d5e9fe8b29b42a4908fb9ed4b0af914":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45714c75b2b0451b97ba48002648c2dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45ab06d8c03a4c97871e45d20dbe6339":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a3e2f0ca4e849b391fc40d531708c7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"546939d1ff514057a3f610445bf1d62b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62530bd82b0249639448eb6637726489":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62ba208a1ffc4e0cb794a25cccbbe120":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c2891c69bd344c4bf03bd0be2af900d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07caa3d01ad64211b0c2faa62678d780","placeholder":"​","style":"IPY_MODEL_23ef0580534e4e3b9174e823e4336b56","value":" 232k/232k [00:00&lt;00:00, 5.95MB/s]"}},"6d6da478249b4751bb5c70a2041faa53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c9300d2bb74c0eb1cb18c524530219":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78e750c788294fa18229e3ed3922bfd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ed7cbfe49824cc0a27fde4dcfbb8bc2","placeholder":"​","style":"IPY_MODEL_a3e1706adc6f4ec985194d8e17c88175","value":" 48.0/48.0 [00:00&lt;00:00, 868B/s]"}},"7921421d0c5542fc9ac1b4268e15ed03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ab65fa365344bbfbab6be79bbfb4c2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e2c78ab77ba4910b3712c1e0af16951":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8678086282854e7c92a04980aa570c29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_953da43ba083434184e994edce05c9ac","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0fabf1f050e41e193e7388645396d45","value":48}},"87fe1eb308e2471faadb4fdbddaa2884":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e1a8ebec6754f8980e12ae8b5ba96b4","placeholder":"​","style":"IPY_MODEL_aef9aee7614b4c6b8745d805d262b30b","value":" 570/570 [00:00&lt;00:00, 15.6kB/s]"}},"882d840c38a54e09bd6d681a78214912":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4d8abd5e0a4046b43adb19dd6671db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b32bf0f9a244383b673cdf96b017905","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0204284f7f24c70822772ec398c7728","value":466062}},"8d2ee842daa74660b1431c6d93a0bbf9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"953da43ba083434184e994edce05c9ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97bb383af4214933a21b2c2e69a9accc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ab65fa365344bbfbab6be79bbfb4c2f","placeholder":"​","style":"IPY_MODEL_62ba208a1ffc4e0cb794a25cccbbe120","value":" 466k/466k [00:00&lt;00:00, 12.8MB/s]"}},"9e1a8ebec6754f8980e12ae8b5ba96b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ea4802221d44167a6233ea3175fcd6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1a2fae51e9542e1ba58fa7eb03a88f4","IPY_MODEL_8678086282854e7c92a04980aa570c29","IPY_MODEL_78e750c788294fa18229e3ed3922bfd1"],"layout":"IPY_MODEL_7921421d0c5542fc9ac1b4268e15ed03"}},"a3e1706adc6f4ec985194d8e17c88175":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aef9aee7614b4c6b8745d805d262b30b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b78c6465e67e4470adf7c8d4b8b62986":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_62530bd82b0249639448eb6637726489","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebc0a0d5bc564278adeaa2dcdea84bdb","value":440449768}},"bb7866841ca7485d9408bbc85135431b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7127db0acf14e7aba570b90837a2131","placeholder":"​","style":"IPY_MODEL_8d2ee842daa74660b1431c6d93a0bbf9","value":"tokenizer.json: 100%"}},"c7127db0acf14e7aba570b90837a2131":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfd726138ced45eab470bc74f4c7a460":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a3e2f0ca4e849b391fc40d531708c7e","placeholder":"​","style":"IPY_MODEL_d17677e7997c47ee83d8b0f1710d852e","value":"vocab.txt: 100%"}},"d0b3e0e190e0441abb84ac6fd0c7cb0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d17677e7997c47ee83d8b0f1710d852e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9795910ee3d467abe13484182aadcbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f5756a9ea60f47d8a46b143a9d3cdc73","IPY_MODEL_0e550590e8dc4b9a8d851a683f31a32f","IPY_MODEL_87fe1eb308e2471faadb4fdbddaa2884"],"layout":"IPY_MODEL_7e2c78ab77ba4910b3712c1e0af16951"}},"de517d37d747469e9466c71f14b334bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df58971bf6cd4f4d99d481ab4d2565b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45714c75b2b0451b97ba48002648c2dc","placeholder":"​","style":"IPY_MODEL_2c9647540566454c961b4185acb5069d","value":" 440M/440M [00:06&lt;00:00, 140MB/s]"}},"e0204284f7f24c70822772ec398c7728":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e06c96e4e2144460aa1e858c9bda34db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfd726138ced45eab470bc74f4c7a460","IPY_MODEL_062d165d39eb4893a30f4e5b95cbbeb8","IPY_MODEL_6c2891c69bd344c4bf03bd0be2af900d"],"layout":"IPY_MODEL_ec692ca7d5164313a9ef676a09f40668"}},"e08a90e438eb4c0e85c9e1ba1d366564":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb7866841ca7485d9408bbc85135431b","IPY_MODEL_8a4d8abd5e0a4046b43adb19dd6671db","IPY_MODEL_97bb383af4214933a21b2c2e69a9accc"],"layout":"IPY_MODEL_6d6da478249b4751bb5c70a2041faa53"}},"e0fabf1f050e41e193e7388645396d45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3347193cd6544418fb37f04fa4e39aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebc0a0d5bc564278adeaa2dcdea84bdb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec692ca7d5164313a9ef676a09f40668":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1a2fae51e9542e1ba58fa7eb03a88f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11fb7e2a02204e749bb7f92ea0044e58","placeholder":"​","style":"IPY_MODEL_73c9300d2bb74c0eb1cb18c524530219","value":"tokenizer_config.json: 100%"}},"f5756a9ea60f47d8a46b143a9d3cdc73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45ab06d8c03a4c97871e45d20dbe6339","placeholder":"​","style":"IPY_MODEL_3d5e9fe8b29b42a4908fb9ed4b0af914","value":"config.json: 100%"}},"f6b2a0f7ace8445ca07357fb54b0df24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0b3e0e190e0441abb84ac6fd0c7cb0c","placeholder":"​","style":"IPY_MODEL_e3347193cd6544418fb37f04fa4e39aa","value":"model.safetensors: 100%"}}}}},"nbformat":4,"nbformat_minor":0}
